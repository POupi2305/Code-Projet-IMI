{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgNsRaRyoeBD",
        "outputId": "88e5287c-29e1-4cd3-917e-d8fa9041dd53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "\n",
        "USE_GPU = True\n",
        "\n",
        "dtype = torch.float64 # we will be using float throughout this tutorial\n",
        "device = torch.device('cuda') if (USE_GPU and torch.cuda.is_available()) else torch.device('cpu')\n",
        "print('using device:', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbAi3cLQjcBN"
      },
      "source": [
        "# The algorithm :\n",
        "\n",
        "***Algorithm 1 :*** American Option Pricing with Multiple Neural Networks\n",
        "\n",
        "**Result :** Functions $\\Phi_{t_i}, \\Psi_{t_i}$ for $i \\in \\{0,1, \\ldots, n-1\\}$\n",
        "\n",
        "Simulate $N$ stock paths\n",
        "\n",
        "Initialize $Y_{t_n}=X_{t_n}=f\\left(S_{t_n}\\right)$\n",
        "\n",
        "for $i=n-1: 1$ do \\\\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; Regress $\\beta_{\\Delta t}^{-1} Y_{t_{i+1}}$ on $S_{t_i}:$ \\\\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\min_{\\Phi_{t_i}, \\Psi_{t_i}}\\left(\\beta_{\\Delta t}^{-1} Y_{t_{i+1}} - \\Phi_{t_i}\\left(S_{t_i}\\right) - \\Psi_{t_i}\\left(S_{t_i}\\right) \\Delta W_{t_i}\\right)^2$ \\\\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; $Y_{t_i} = \\beta_{\\Delta t}^{-1} Y_{t_{i+1}} - \\Psi_{t_i}\\left(S_{t_i}\\right) \\Delta W_{t_i}$ \\\\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; $X_{t_i} = \\beta_{\\Delta t}^{-1} X_{t_{i+1}} - \\Psi_{t_i}\\left(S_{t_i}\\right) \\Delta W_{t_i}$ \\\\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; if $f\\left(S_{t_i}\\right) > \\Phi_{t_i}\\left(S_{t_i}\\right)$ then \\\\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $Y_{t_i} = f\\left(S_{t_i}\\right)$ \\\\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; end \\\\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; if $f\\left(S_{t_i}\\right) > X_{t_i}$ then \\\\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $X_{t_i} = f\\left(S_{t_i}\\right)$ \\\\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; end \\\\\n",
        "end\n",
        "\n",
        "Regress $\\beta_{\\Delta t}^{-1} Y_{t_1}$ on $S_{t_0}:$ \\\\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; $\\min \\left(\\beta_{\\Delta t}^{-1} Y_{t_1} - \\Phi_{t_0}\\left(S_{t_0}\\right) - \\Psi_{t_0}\\left(S_{t_0}\\right) \\Delta W_{t_0}\\right)^2$ \\\\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; $Y_{t_0} = \\beta_{\\Delta t}^{-1} Y_{t_1} - \\Psi_{t_0}\\left(S_{t_0}\\right) \\Delta W_{t_0}$ \\\\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; $X_{t_0} = \\beta_{\\Delta t}^{-1} X_{t_1} - \\Psi_{t_0}\\left(S_{t_0}\\right) \\Delta W_{t_0}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQ_yOEOQ_bsD"
      },
      "source": [
        "## American Option Pricing with Multiple Neural Networks (method 1) article [1]\n",
        "Here I'll try a simple implementation of the method I of the first article :\n",
        "\n",
        "Here we have constant interest rate so the discount factor is $\\exp(-rT)$, and the stock dynamics are modelled with Geometric Brownian Motion (GBM).\n",
        "\n",
        "$\\large dS_t = rS_tdt+\\sigma S_tdW_t$\n",
        "\n",
        "Let's simulate this GBM process by simulating variables of the natural logarithm process of the stock price $x_t = ln(S_t)$, which is normally distributed. For the dynamics of the natural logarithm process of stock prices under GBM model you need to use Ito's calculus.\n",
        "$\\large dx_t = \\nu dt+\\sigma dW_t ,  \\nu = r - \\frac{1}{2} \\sigma ^ 2$\n",
        "\n",
        "We can then discretize the Stochastic Differential Equation (SDE) by changing the infinitesimals $dx, dt, dz$ into small steps $\\Delta x, \\Delta t, \\Delta W$.\n",
        "\n",
        "$\\large \\Delta x = \\nu  \\Delta t+\\sigma \\Delta W$\n",
        "\n",
        "This is the exact solution to the SDE and involves no approximation.\n",
        "\n",
        "$\\large x_{t+\\Delta t} = x_{t} + \\nu (\\Delta t)+\\sigma (W_{t+\\Delta t}- W_t)$\n",
        "\n",
        "In terms of the stock price S, we have:\n",
        "\n",
        "$\\large S_{t+\\Delta t} = S_{t} \\exp( \\nu \\Delta t + \\sigma (W_{t+\\Delta t}- W_t) )$\n",
        "\n",
        "Where $(W_{t+\\Delta t}- W_t) \\sim N(0,\\Delta t) \\sim \\sqrt{\\Delta t} N(0,1) \\sim \\sqrt{\\Delta t} \\epsilon_i$\n",
        "\n",
        "\n",
        "\\\\\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "8-aSQ4Kp_fgD"
      },
      "outputs": [],
      "source": [
        "def pay_off(S,r,K,dt):\n",
        "  return torch.maximum(K- S,torch.zeros_like(S))\n",
        "\n",
        "def simulate_paths(M, T, n, r, vol, S, K, dt):\n",
        "    nudt = (r - 0.5 * vol**2) * dt\n",
        "    lnS = np.log(S)\n",
        "    # Méthode de Monte Carlo\n",
        "    Z = np.random.normal(size=(M,n))\n",
        "    dW=np.sqrt(dt) * Z  #it's the simulation of the dWt_i we'll need in each iteration\n",
        "    delta_lnSt = nudt + vol*dW\n",
        "\n",
        "    LnS_s = np.zeros([M, n + 1])\n",
        "\n",
        "    # Set the initial values\n",
        "    LnS_s[:, 0] = lnS\n",
        "    # Compute cumulative sums using cumsum\n",
        "    LnS_s[:, 1:] = np.cumsum(delta_lnSt, axis=1)\n",
        "    LnS_s[:,1:] +=lnS\n",
        "\n",
        "\n",
        "    S = np.exp(LnS_s)\n",
        "    S_tensor = torch.tensor(S, device = device ,dtype=dtype)\n",
        "    dW_tensor = torch.tensor(dW, device = device ,dtype=dtype)\n",
        "    return S_tensor, dW_tensor\n",
        "\n",
        "#Parametres\n",
        "T = 1\n",
        "n=50\n",
        "dt = T/n  #les t_i seront donc les i*dt.\n",
        "\n",
        "\n",
        "S = 36          # Prix de l'action\n",
        "K = 40           # Prix d'exercice\n",
        "vol = 0.2       # Volatilité (%)\n",
        "r = 0.06            # Taux sans risque (%)\n",
        "M = 100000 # Nombre de simulations\n",
        "\n",
        "\n",
        "S,dW=simulate_paths(M, T, n, r, vol, S, K, dt)\n",
        "X=torch.zeros([M,n+1], device = device ,dtype=dtype)\n",
        "Y=torch.zeros_like(X, device = device ,dtype=dtype)\n",
        "\n",
        "\n",
        "X[ :, n]=pay_off(S[:,n],r,K,dt)\n",
        "Y[ :, n]=X[ :, n]\n",
        "\n",
        "beta_dt=math.exp(-r*dt)\n",
        "epsilon = 1e-8 #for all division tasks\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleModel1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleModel1, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(2, 50),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(50, 50),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(50, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class SimpleModel2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleModel2, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(1, 50),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(50, 50),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(50, 50),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(50, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class ModelManager():\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
        "\n",
        "    def train(self, i , inputs, outputs, epochs=5, batch_size=32, validation_split=0.2):\n",
        "        torch.cuda.empty_cache()\n",
        "        dataset = TensorDataset(inputs, outputs)\n",
        "        train_size = int((1 - validation_split) * len(dataset))\n",
        "        val_size = len(dataset) - train_size\n",
        "        train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "        self.model.train()\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            for batch_inputs, batch_labels in train_dataloader:\n",
        "                self.optimizer.zero_grad()\n",
        "                outputs = self.model(batch_inputs)\n",
        "                loss = self.loss_fn(outputs, batch_labels)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "\n",
        "\n",
        "            val_loss = 0\n",
        "            if epoch % 3 == 0:\n",
        "                self.model.eval()\n",
        "                with torch.no_grad():\n",
        "                    for S_batch, labels_batch in val_dataloader:\n",
        "                          outputs = self.model(S_batch)\n",
        "                          val_loss += self.loss_fn(labels_batch, outputs).item()\n",
        "\n",
        "                print(f\"for the {i} th iteration and epoch {epoch+1}/{epochs}, Training Loss: {total_loss/len(train_dataset)}, Validation Loss: {val_loss/val_size}\")\n",
        "\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(inputs)\n",
        "        return outputs\n",
        "\n",
        "model1 = SimpleModel1().to(device = device ,dtype=dtype)\n",
        "model2 = SimpleModel2().to(device = device ,dtype=dtype)\n",
        "\n",
        "manager1 = ModelManager(model1)\n",
        "manager2 = ModelManager(model2)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gdlrnUZ0EpLm"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(n-1,0,-1):\n",
        "    #determination of phi and psi in two steps\n",
        "\n",
        "    # model 1\n",
        "    concatenated_inputs= torch.concatenate([S[:,i].reshape(-1,1),dW[:,i].reshape(-1,1)],axis=1)\n",
        "    manager1.train(i ,concatenated_inputs,beta_dt*Y[:,i+1], epochs=10, batch_size=15)\n",
        "    general_fct_S_W = manager1.predict(concatenated_inputs) # Y ~ f(S,W) #as a function of S, W\n",
        "\n",
        "    #transformation on the outputs\n",
        "    concatenated_inputs_0 = torch.concatenate([S[:,i].reshape(-1,1),torch.zeros_like(dW[:,i].reshape(-1,1))],axis=1) #or just torch.zeros(M,1)\n",
        "    phi_S = manager1.predict(concatenated_inputs_0) # predit f(S, 0) = phi(S)\n",
        "    psi_S_prime= (general_fct_S_W-phi_S)/(dW[:,i]+epsilon).reshape(-1,1) # (f(S,W)-phi(S))/W\n",
        "\n",
        "    # model 2\n",
        "    manager2.train(i , S[:,i].reshape(-1,1),psi_S_prime, epochs=10, batch_size=15) # (f(S,W)-phi(S))/W ~ g(S) as a function of g(S), we are litteraly computing the df/dW(S,0) !\n",
        "    psi_S= manager2.predict(S[:,i].reshape(-1,1)) #and now g(S) = psi(S)\n",
        "\n",
        "    X[ :, i]=beta_dt*X[:,i+1]-psi_S.reshape(M) *dW[:,i]\n",
        "    Y[ :, i]=beta_dt*Y[:,i+1]-psi_S.reshape(M) *dW[:,i]\n",
        "    Z=pay_off(S[:,i],r,K,dt)\n",
        "\n",
        "\n",
        "    Y[ :, i] = torch.where(Z> phi_S.reshape(M), Z, Y[ :, i])\n",
        "    X[ :, i] = torch.where(Z> X[:,i], Z, X[ :, i])\n",
        "\n",
        "\n",
        "#Pour i =0 :\n",
        "#determination of phi and psi in two steps\n",
        "i =0\n",
        "# model 1\n",
        "concatenated_inputs= torch.concatenate([S[:,i].reshape(-1,1),dW[:,i].reshape(-1,1)],axis=1)\n",
        "manager1.train(i , concatenated_inputs,beta_dt*Y[:,i+1], epochs=10, batch_size=15)\n",
        "general_fct_S_W = manager1.predict(concatenated_inputs)\n",
        "\n",
        "#transformation on the outputs\n",
        "concatenated_inputs_0= torch.concatenate([S[:,i].reshape(-1,1),torch.zeros_like(dW[:,i].reshape(-1,1))],axis=1)\n",
        "phi_S = manager1.predict(concatenated_inputs_0)\n",
        "psi_S_prime= (general_fct_S_W-phi_S)/(dW[:,i]+epsilon).reshape(-1,1)\n",
        "\n",
        "# model 2\n",
        "manager2.train(i, S[:,i].reshape(-1,1),psi_S_prime, epochs=10, batch_size=15)\n",
        "psi_S= manager2.predict(S[:,i].reshape(-1,1))\n",
        "\n",
        "X[ :, i]=beta_dt*X[:,i+1]-psi_S.reshape(M) *dW[:,i]\n",
        "Y[ :, i]=beta_dt*Y[:,i+1]-psi_S.reshape(M) *dW[:,i]"
      ],
      "metadata": {
        "id": "ElLTcc68DWbI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49cba130-df62-463d-8a1f-3d1f648602e2"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([15])) that is different to the input size (torch.Size([15, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([15, 1])) that is different to the input size (torch.Size([15])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([5, 1])) that is different to the input size (torch.Size([5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "for the 49 th iteration and epoch 1/10, Training Loss: 2.50501713244713, Validation Loss: 2.125764443026513\n",
            "for the 49 th iteration and epoch 4/10, Training Loss: 1.4170994822364824, Validation Loss: 1.9431932053762941\n",
            "for the 49 th iteration and epoch 7/10, Training Loss: 1.3352874569230448, Validation Loss: 2.020372720593801\n",
            "for the 49 th iteration and epoch 10/10, Training Loss: 1.4834127192226305, Validation Loss: 1.9416574384914154\n",
            "for the 49 th iteration and epoch 1/10, Training Loss: 0.06875064680406713, Validation Loss: 0.05499187221114125\n",
            "for the 49 th iteration and epoch 4/10, Training Loss: 0.0018758473840112429, Validation Loss: 0.00613116815156593\n",
            "for the 49 th iteration and epoch 7/10, Training Loss: 0.00031747501065201595, Validation Loss: 0.000884827270101568\n",
            "for the 49 th iteration and epoch 10/10, Training Loss: 7.365965221000593e-05, Validation Loss: 7.804114813050447e-05\n",
            "for the 48 th iteration and epoch 1/10, Training Loss: 1.377566847010927, Validation Loss: 1.8139325189483728\n",
            "for the 48 th iteration and epoch 4/10, Training Loss: 1.4287856125030063, Validation Loss: 1.811834055790205\n",
            "for the 48 th iteration and epoch 7/10, Training Loss: 1.3200510314227938, Validation Loss: 1.8103317393338059\n",
            "for the 48 th iteration and epoch 10/10, Training Loss: 1.3681582739448057, Validation Loss: 1.810540317435611\n",
            "for the 48 th iteration and epoch 1/10, Training Loss: 2.1591255210485466e-05, Validation Loss: 6.661402278267508e-05\n",
            "for the 48 th iteration and epoch 4/10, Training Loss: 6.30319239011181e-06, Validation Loss: 3.3275582304264523e-06\n",
            "for the 48 th iteration and epoch 7/10, Training Loss: 1.6877219734931055e-06, Validation Loss: 2.2607120753476783e-06\n",
            "for the 48 th iteration and epoch 10/10, Training Loss: 1.0530893839136619e-06, Validation Loss: 7.061220479095451e-07\n",
            "for the 47 th iteration and epoch 1/10, Training Loss: 1.6834971007900763, Validation Loss: 1.24683613220927\n",
            "for the 47 th iteration and epoch 4/10, Training Loss: 1.4938638363638663, Validation Loss: 1.2566078858162288\n",
            "for the 47 th iteration and epoch 7/10, Training Loss: 1.4927448908412098, Validation Loss: 1.245544317506503\n",
            "for the 47 th iteration and epoch 10/10, Training Loss: 1.3923065929069542, Validation Loss: 1.2453948537224624\n",
            "for the 47 th iteration and epoch 1/10, Training Loss: 3.032169720232858e-06, Validation Loss: 3.84059054893057e-06\n",
            "for the 47 th iteration and epoch 4/10, Training Loss: 5.50415193665437e-07, Validation Loss: 1.0768193574874084e-07\n",
            "for the 47 th iteration and epoch 7/10, Training Loss: 1.5846355122932657e-07, Validation Loss: 1.3893035598828949e-07\n",
            "for the 47 th iteration and epoch 10/10, Training Loss: 1.0698656798463059e-07, Validation Loss: 9.282969895918357e-08\n",
            "for the 46 th iteration and epoch 1/10, Training Loss: 1.2978773597900843, Validation Loss: 1.755583706678184\n",
            "for the 46 th iteration and epoch 4/10, Training Loss: 1.2931457391430599, Validation Loss: 1.6963707744864287\n",
            "for the 46 th iteration and epoch 7/10, Training Loss: 1.2537943594930612, Validation Loss: 1.6884825879108654\n",
            "for the 46 th iteration and epoch 10/10, Training Loss: 1.3508944729686871, Validation Loss: 1.724171495431317\n",
            "for the 46 th iteration and epoch 1/10, Training Loss: 1.1090146238947424e-06, Validation Loss: 1.0948440252086446e-06\n",
            "for the 46 th iteration and epoch 4/10, Training Loss: 4.146034486626928e-07, Validation Loss: 1.1255400104892208e-06\n",
            "for the 46 th iteration and epoch 7/10, Training Loss: 2.2208635196335982e-07, Validation Loss: 5.674523645866648e-07\n",
            "for the 46 th iteration and epoch 10/10, Training Loss: 2.5512118286864544e-07, Validation Loss: 6.810202793604258e-07\n",
            "for the 45 th iteration and epoch 1/10, Training Loss: 1.4293844927110828, Validation Loss: 1.3569051676561026\n",
            "for the 45 th iteration and epoch 4/10, Training Loss: 1.4014002804519385, Validation Loss: 1.2663339929307842\n",
            "for the 45 th iteration and epoch 7/10, Training Loss: 1.5903815893727695, Validation Loss: 1.3135629187897746\n",
            "for the 45 th iteration and epoch 10/10, Training Loss: 1.4622272016388371, Validation Loss: 1.268092764731851\n",
            "for the 45 th iteration and epoch 1/10, Training Loss: 4.5382038690612604e-05, Validation Loss: 5.5200138284565026e-05\n",
            "for the 45 th iteration and epoch 4/10, Training Loss: 7.845324599125784e-06, Validation Loss: 1.7270004386921733e-06\n",
            "for the 45 th iteration and epoch 7/10, Training Loss: 3.1410264163424145e-06, Validation Loss: 2.4416543642004244e-06\n",
            "for the 45 th iteration and epoch 10/10, Training Loss: 2.1058116583017817e-06, Validation Loss: 2.4175050716357897e-06\n",
            "for the 44 th iteration and epoch 1/10, Training Loss: 1.1991220216295575, Validation Loss: 1.3526776853669682\n",
            "for the 44 th iteration and epoch 4/10, Training Loss: 1.2248946213316514, Validation Loss: 1.361389535179038\n",
            "for the 44 th iteration and epoch 7/10, Training Loss: 1.2103386313347162, Validation Loss: 1.3466159852652653\n",
            "for the 44 th iteration and epoch 10/10, Training Loss: 1.1394147024079233, Validation Loss: 1.3655176151663846\n",
            "for the 44 th iteration and epoch 1/10, Training Loss: 1.4235397495521475e-05, Validation Loss: 1.0140085516564758e-05\n",
            "for the 44 th iteration and epoch 4/10, Training Loss: 5.319114795614391e-06, Validation Loss: 1.0331544787903894e-05\n",
            "for the 44 th iteration and epoch 7/10, Training Loss: 4.746489001099381e-06, Validation Loss: 1.2341295181913137e-05\n",
            "for the 44 th iteration and epoch 10/10, Training Loss: 3.5539483834479583e-06, Validation Loss: 6.1360789854460195e-06\n",
            "for the 43 th iteration and epoch 1/10, Training Loss: 1.5037220038193078, Validation Loss: 0.8306742745241852\n",
            "for the 43 th iteration and epoch 4/10, Training Loss: 1.408286565310302, Validation Loss: 0.8498640094257928\n",
            "for the 43 th iteration and epoch 7/10, Training Loss: 1.4057506650522398, Validation Loss: 0.8371456380257285\n",
            "for the 43 th iteration and epoch 10/10, Training Loss: 1.2820185543682074, Validation Loss: 0.8169056157094797\n",
            "for the 43 th iteration and epoch 1/10, Training Loss: 8.194144830329387e-06, Validation Loss: 7.683371142422328e-06\n",
            "for the 43 th iteration and epoch 4/10, Training Loss: 3.414903505861565e-06, Validation Loss: 6.1480427594804685e-06\n",
            "for the 43 th iteration and epoch 7/10, Training Loss: 2.3139069391080414e-06, Validation Loss: 1.5171824104397069e-06\n",
            "for the 43 th iteration and epoch 10/10, Training Loss: 2.439534061798542e-06, Validation Loss: 1.4125789678650424e-06\n",
            "for the 42 th iteration and epoch 1/10, Training Loss: 1.4120629502082198, Validation Loss: 1.366074725921625\n",
            "for the 42 th iteration and epoch 4/10, Training Loss: 1.2675628481149568, Validation Loss: 1.3759717041239687\n",
            "for the 42 th iteration and epoch 7/10, Training Loss: 1.3481213440161928, Validation Loss: 1.429698145865208\n",
            "for the 42 th iteration and epoch 10/10, Training Loss: 1.320071641611494, Validation Loss: 1.390886412117134\n",
            "for the 42 th iteration and epoch 1/10, Training Loss: 8.375632277289687e-06, Validation Loss: 5.645573175963445e-06\n",
            "for the 42 th iteration and epoch 4/10, Training Loss: 3.4556953370607794e-06, Validation Loss: 4.243361246860622e-06\n",
            "for the 42 th iteration and epoch 7/10, Training Loss: 3.0779609280299762e-06, Validation Loss: 3.824894309090003e-06\n",
            "for the 42 th iteration and epoch 10/10, Training Loss: 3.524352398978218e-06, Validation Loss: 4.43849450455912e-06\n",
            "for the 41 th iteration and epoch 1/10, Training Loss: 1.2640233526130613, Validation Loss: 1.5946438929329343\n",
            "for the 41 th iteration and epoch 4/10, Training Loss: 1.3027494982148105, Validation Loss: 1.4331121799935538\n",
            "for the 41 th iteration and epoch 7/10, Training Loss: 1.2734437711845854, Validation Loss: 1.4332244111975085\n",
            "for the 41 th iteration and epoch 10/10, Training Loss: 1.2179968942062884, Validation Loss: 1.490764816569238\n",
            "for the 41 th iteration and epoch 1/10, Training Loss: 7.890584144974375e-06, Validation Loss: 6.710786300165338e-06\n",
            "for the 41 th iteration and epoch 4/10, Training Loss: 4.922383088414179e-06, Validation Loss: 6.673898407334619e-06\n",
            "for the 41 th iteration and epoch 7/10, Training Loss: 5.341262306634916e-06, Validation Loss: 7.803434792682924e-06\n",
            "for the 41 th iteration and epoch 10/10, Training Loss: 6.712937165571828e-06, Validation Loss: 3.408431507351267e-06\n",
            "for the 40 th iteration and epoch 1/10, Training Loss: 1.2134267512492503, Validation Loss: 1.5993326148493465\n",
            "for the 40 th iteration and epoch 4/10, Training Loss: 1.227274269283262, Validation Loss: 1.6515260411688941\n",
            "for the 40 th iteration and epoch 7/10, Training Loss: 1.2351833063592086, Validation Loss: 1.5758466074373216\n",
            "for the 40 th iteration and epoch 10/10, Training Loss: 1.436203430656327, Validation Loss: 1.5002615393367704\n",
            "for the 40 th iteration and epoch 1/10, Training Loss: 6.3932541264182506e-06, Validation Loss: 1.886521814249683e-05\n",
            "for the 40 th iteration and epoch 4/10, Training Loss: 3.844613511126494e-06, Validation Loss: 4.465503691282026e-06\n",
            "for the 40 th iteration and epoch 7/10, Training Loss: 3.106128605875932e-06, Validation Loss: 3.305658864047327e-06\n",
            "for the 40 th iteration and epoch 10/10, Training Loss: 4.035736666228543e-06, Validation Loss: 1.9479827841630636e-06\n",
            "for the 39 th iteration and epoch 1/10, Training Loss: 1.280943460598899, Validation Loss: 1.646873534932837\n",
            "for the 39 th iteration and epoch 4/10, Training Loss: 1.318333988342988, Validation Loss: 1.6655102397811121\n",
            "for the 39 th iteration and epoch 7/10, Training Loss: 1.3195585902481342, Validation Loss: 1.646793351427096\n",
            "for the 39 th iteration and epoch 10/10, Training Loss: 1.2730020767293275, Validation Loss: 1.6782709970570509\n",
            "for the 39 th iteration and epoch 1/10, Training Loss: 2.8995286012994086e-06, Validation Loss: 3.0066067027975193e-06\n",
            "for the 39 th iteration and epoch 4/10, Training Loss: 2.707533518330365e-06, Validation Loss: 2.3440271045723584e-06\n",
            "for the 39 th iteration and epoch 7/10, Training Loss: 2.130495115400878e-06, Validation Loss: 3.0786269791623883e-06\n",
            "for the 39 th iteration and epoch 10/10, Training Loss: 2.0408323010633117e-06, Validation Loss: 3.1374936116863273e-06\n",
            "for the 38 th iteration and epoch 1/10, Training Loss: 1.321196789790607, Validation Loss: 1.2521095468495091\n",
            "for the 38 th iteration and epoch 4/10, Training Loss: 1.3284743124507703, Validation Loss: 1.265608116587702\n",
            "for the 38 th iteration and epoch 7/10, Training Loss: 1.3380435427576745, Validation Loss: 1.267592418796573\n",
            "for the 38 th iteration and epoch 10/10, Training Loss: 1.3250283289558005, Validation Loss: 1.3243205705917087\n",
            "for the 38 th iteration and epoch 1/10, Training Loss: 3.459611683518998e-06, Validation Loss: 3.413504085188446e-06\n",
            "for the 38 th iteration and epoch 4/10, Training Loss: 1.290066084251219e-06, Validation Loss: 1.068501415475176e-06\n",
            "for the 38 th iteration and epoch 7/10, Training Loss: 8.497188773459986e-07, Validation Loss: 2.500789970355856e-06\n",
            "for the 38 th iteration and epoch 10/10, Training Loss: 7.227804668442987e-07, Validation Loss: 1.0347572235798912e-06\n",
            "for the 37 th iteration and epoch 1/10, Training Loss: 1.3733588876346234, Validation Loss: 1.7631636737549854\n",
            "for the 37 th iteration and epoch 4/10, Training Loss: 1.4124865191237812, Validation Loss: 1.7618824641064208\n",
            "for the 37 th iteration and epoch 7/10, Training Loss: 1.3129797229198938, Validation Loss: 1.7867424882093463\n",
            "for the 37 th iteration and epoch 10/10, Training Loss: 1.6005076109965786, Validation Loss: 1.7812950982545455\n",
            "for the 37 th iteration and epoch 1/10, Training Loss: 3.3892559703633696e-06, Validation Loss: 1.0207592658389808e-06\n",
            "for the 37 th iteration and epoch 4/10, Training Loss: 1.5329484794967441e-06, Validation Loss: 4.566360341034814e-06\n",
            "for the 37 th iteration and epoch 7/10, Training Loss: 1.115050494050862e-06, Validation Loss: 2.149761239387221e-07\n",
            "for the 37 th iteration and epoch 10/10, Training Loss: 4.51808864042417e-07, Validation Loss: 4.307563482958375e-07\n",
            "for the 36 th iteration and epoch 1/10, Training Loss: 1.2530170087502197, Validation Loss: 1.3646460820741495\n",
            "for the 36 th iteration and epoch 4/10, Training Loss: 1.32901004763113, Validation Loss: 1.4128223722892173\n",
            "for the 36 th iteration and epoch 7/10, Training Loss: 1.3948044130036115, Validation Loss: 1.5621081772283838\n",
            "for the 36 th iteration and epoch 10/10, Training Loss: 1.2745182008320861, Validation Loss: 1.494871009145713\n",
            "for the 36 th iteration and epoch 1/10, Training Loss: 1.2573624952728902e-05, Validation Loss: 5.158647422880538e-06\n",
            "for the 36 th iteration and epoch 4/10, Training Loss: 3.86788609157084e-06, Validation Loss: 4.049501269521794e-06\n",
            "for the 36 th iteration and epoch 7/10, Training Loss: 2.4148536590922577e-06, Validation Loss: 2.2537862353910925e-06\n",
            "for the 36 th iteration and epoch 10/10, Training Loss: 1.6520303282973826e-06, Validation Loss: 2.6116143265910657e-06\n",
            "for the 35 th iteration and epoch 1/10, Training Loss: 1.316609748408673, Validation Loss: 1.3617447516360626\n",
            "for the 35 th iteration and epoch 4/10, Training Loss: 1.339411405505808, Validation Loss: 1.3850178356785556\n",
            "for the 35 th iteration and epoch 7/10, Training Loss: 1.6946182063572999, Validation Loss: 1.3648164098139355\n",
            "for the 35 th iteration and epoch 10/10, Training Loss: 1.442673005592959, Validation Loss: 1.3593807904360926\n",
            "for the 35 th iteration and epoch 1/10, Training Loss: 3.946699595425936e-06, Validation Loss: 2.208065370553366e-06\n",
            "for the 35 th iteration and epoch 4/10, Training Loss: 1.8511172208414808e-06, Validation Loss: 2.3205860981119507e-06\n",
            "for the 35 th iteration and epoch 7/10, Training Loss: 1.8810509457363623e-06, Validation Loss: 2.6143854228323773e-06\n",
            "for the 35 th iteration and epoch 10/10, Training Loss: 1.547720090666386e-06, Validation Loss: 2.0287048600980665e-06\n",
            "for the 34 th iteration and epoch 1/10, Training Loss: 1.1161280441000245, Validation Loss: 1.762995798370931\n",
            "for the 34 th iteration and epoch 4/10, Training Loss: 1.1865639683918987, Validation Loss: 1.7868447690712805\n",
            "for the 34 th iteration and epoch 7/10, Training Loss: 1.285633947078076, Validation Loss: 1.8030723615909985\n",
            "for the 34 th iteration and epoch 10/10, Training Loss: 1.230368055682077, Validation Loss: 1.7653199978205527\n",
            "for the 34 th iteration and epoch 1/10, Training Loss: 1.1167734027720227e-05, Validation Loss: 8.012084793638134e-06\n",
            "for the 34 th iteration and epoch 4/10, Training Loss: 7.96009480240055e-06, Validation Loss: 5.183005768864264e-06\n",
            "for the 34 th iteration and epoch 7/10, Training Loss: 3.691454134146402e-06, Validation Loss: 1.3064202024591838e-05\n",
            "for the 34 th iteration and epoch 10/10, Training Loss: 3.1586356314600553e-06, Validation Loss: 4.398331280817927e-06\n",
            "for the 33 th iteration and epoch 1/10, Training Loss: 1.1214589000132942, Validation Loss: 1.1662575308959406\n",
            "for the 33 th iteration and epoch 4/10, Training Loss: 1.2871985998024833, Validation Loss: 1.2388311327426695\n",
            "for the 33 th iteration and epoch 7/10, Training Loss: 1.0800342454703944, Validation Loss: 1.2270544187249492\n",
            "for the 33 th iteration and epoch 10/10, Training Loss: 1.2326868695155642, Validation Loss: 1.1761798591602104\n",
            "for the 33 th iteration and epoch 1/10, Training Loss: 8.052677977505435e-05, Validation Loss: 3.0823913957757884e-05\n",
            "for the 33 th iteration and epoch 4/10, Training Loss: 3.189946420640448e-05, Validation Loss: 4.197902692001693e-05\n",
            "for the 33 th iteration and epoch 7/10, Training Loss: 1.2221045344009065e-05, Validation Loss: 1.0964328434023285e-05\n",
            "for the 33 th iteration and epoch 10/10, Training Loss: 1.3270029752923939e-05, Validation Loss: 1.0013264929040295e-05\n",
            "for the 32 th iteration and epoch 1/10, Training Loss: 0.9207408564470845, Validation Loss: 2.401142084666059\n",
            "for the 32 th iteration and epoch 4/10, Training Loss: 0.9668824044590825, Validation Loss: 2.3868626286684638\n",
            "for the 32 th iteration and epoch 7/10, Training Loss: 1.0063531913579435, Validation Loss: 2.511018640404018\n",
            "for the 32 th iteration and epoch 10/10, Training Loss: 1.0402848661487365, Validation Loss: 2.380106627427878\n",
            "for the 32 th iteration and epoch 1/10, Training Loss: 2.8307627162507523e-05, Validation Loss: 5.7024101564517804e-05\n",
            "for the 32 th iteration and epoch 4/10, Training Loss: 5.355823657258597e-05, Validation Loss: 0.00010342096518396464\n",
            "for the 32 th iteration and epoch 7/10, Training Loss: 2.1366176250275854e-05, Validation Loss: 1.7715086860172503e-05\n",
            "for the 32 th iteration and epoch 10/10, Training Loss: 1.614282239376626e-05, Validation Loss: 1.630369517992461e-05\n",
            "for the 31 th iteration and epoch 1/10, Training Loss: 1.066470607841413, Validation Loss: 1.2282933182444313\n",
            "for the 31 th iteration and epoch 4/10, Training Loss: 1.0871838810727024, Validation Loss: 1.1417086396932312\n",
            "for the 31 th iteration and epoch 7/10, Training Loss: 1.2181999418056222, Validation Loss: 1.2254839064168077\n",
            "for the 31 th iteration and epoch 10/10, Training Loss: 1.1522922755548985, Validation Loss: 1.455990976161856\n",
            "for the 31 th iteration and epoch 1/10, Training Loss: 1.3960986576570173e-05, Validation Loss: 1.254834661388297e-05\n",
            "for the 31 th iteration and epoch 4/10, Training Loss: 1.2783639399968423e-05, Validation Loss: 1.3111587260161881e-05\n",
            "for the 31 th iteration and epoch 7/10, Training Loss: 8.851380399791093e-06, Validation Loss: 1.738796825803482e-05\n",
            "for the 31 th iteration and epoch 10/10, Training Loss: 8.051815452840186e-06, Validation Loss: 1.3264708230552975e-05\n",
            "for the 30 th iteration and epoch 1/10, Training Loss: 1.0802041643682896, Validation Loss: 1.4303547279269844\n",
            "for the 30 th iteration and epoch 4/10, Training Loss: 1.1339319199772966, Validation Loss: 1.3733343603061434\n",
            "for the 30 th iteration and epoch 7/10, Training Loss: 1.1092076006254148, Validation Loss: 1.3304490254985435\n",
            "for the 30 th iteration and epoch 10/10, Training Loss: 1.0693292435403978, Validation Loss: 1.5036759570746046\n",
            "for the 30 th iteration and epoch 1/10, Training Loss: 2.3125324507366023e-05, Validation Loss: 6.588584972003855e-05\n",
            "for the 30 th iteration and epoch 4/10, Training Loss: 4.431759248444627e-05, Validation Loss: 6.719186346328342e-05\n",
            "for the 30 th iteration and epoch 7/10, Training Loss: 2.536426701115169e-05, Validation Loss: 3.0252638750014596e-05\n",
            "for the 30 th iteration and epoch 10/10, Training Loss: 1.4687541543238818e-05, Validation Loss: 2.1218921666770477e-05\n",
            "for the 29 th iteration and epoch 1/10, Training Loss: 1.1780713092256838, Validation Loss: 0.7857167036561934\n",
            "for the 29 th iteration and epoch 4/10, Training Loss: 1.1524310711502277, Validation Loss: 0.7982671953130538\n",
            "for the 29 th iteration and epoch 7/10, Training Loss: 1.339727822581056, Validation Loss: 0.7801389642006425\n",
            "for the 29 th iteration and epoch 10/10, Training Loss: 1.232085451999645, Validation Loss: 0.7702192097450549\n",
            "for the 29 th iteration and epoch 1/10, Training Loss: 6.786451377068513e-05, Validation Loss: 7.537374553998616e-05\n",
            "for the 29 th iteration and epoch 4/10, Training Loss: 3.121267303979394e-05, Validation Loss: 7.723013378778351e-05\n",
            "for the 29 th iteration and epoch 7/10, Training Loss: 3.746077478796779e-05, Validation Loss: 6.240991570773325e-05\n",
            "for the 29 th iteration and epoch 10/10, Training Loss: 3.6111240000488796e-05, Validation Loss: 2.2517080147533522e-05\n",
            "for the 28 th iteration and epoch 1/10, Training Loss: 1.133072210047846, Validation Loss: 1.4158567050310527\n",
            "for the 28 th iteration and epoch 4/10, Training Loss: 1.1134217832645101, Validation Loss: 1.5138513015583446\n",
            "for the 28 th iteration and epoch 7/10, Training Loss: 1.1535474515226203, Validation Loss: 1.4223023381437065\n",
            "for the 28 th iteration and epoch 10/10, Training Loss: 1.081992953282406, Validation Loss: 1.5388155332428384\n",
            "for the 28 th iteration and epoch 1/10, Training Loss: 5.0210545623413945e-05, Validation Loss: 3.367432854830358e-05\n",
            "for the 28 th iteration and epoch 4/10, Training Loss: 2.457715027234479e-05, Validation Loss: 1.014735440597417e-05\n",
            "for the 28 th iteration and epoch 7/10, Training Loss: 1.6609203213477722e-05, Validation Loss: 3.392952881319071e-05\n",
            "for the 28 th iteration and epoch 10/10, Training Loss: 1.4777702383525737e-05, Validation Loss: 1.6241588410508217e-05\n",
            "for the 27 th iteration and epoch 1/10, Training Loss: 1.115422911743344, Validation Loss: 1.3454576377929888\n",
            "for the 27 th iteration and epoch 4/10, Training Loss: 1.1419617719773463, Validation Loss: 1.2954646648356154\n",
            "for the 27 th iteration and epoch 7/10, Training Loss: 1.0120997557693312, Validation Loss: 1.3397237796228667\n",
            "for the 27 th iteration and epoch 10/10, Training Loss: 1.0857659046302468, Validation Loss: 1.3087523866522823\n",
            "for the 27 th iteration and epoch 1/10, Training Loss: 1.6830885639224097e-05, Validation Loss: 2.8227008544937614e-05\n",
            "for the 27 th iteration and epoch 4/10, Training Loss: 1.6640201268954018e-05, Validation Loss: 6.805222440985313e-05\n",
            "for the 27 th iteration and epoch 7/10, Training Loss: 1.5109979998395773e-05, Validation Loss: 2.1412504604594994e-05\n",
            "for the 27 th iteration and epoch 10/10, Training Loss: 2.1898338733666407e-05, Validation Loss: 1.5906992729033845e-05\n",
            "for the 26 th iteration and epoch 1/10, Training Loss: 1.2176720542254629, Validation Loss: 1.2109710640538762\n",
            "for the 26 th iteration and epoch 4/10, Training Loss: 1.0871843824380878, Validation Loss: 1.2337912505516275\n",
            "for the 26 th iteration and epoch 7/10, Training Loss: 1.228884990314624, Validation Loss: 1.234220807235276\n",
            "for the 26 th iteration and epoch 10/10, Training Loss: 1.165517292312012, Validation Loss: 1.210752671393185\n",
            "for the 26 th iteration and epoch 1/10, Training Loss: 1.0159760164477037e-05, Validation Loss: 1.2701708387349462e-05\n",
            "for the 26 th iteration and epoch 4/10, Training Loss: 1.1986379980072718e-05, Validation Loss: 1.1668552839639804e-05\n",
            "for the 26 th iteration and epoch 7/10, Training Loss: 1.0713341849555566e-05, Validation Loss: 1.0758261242451168e-05\n",
            "for the 26 th iteration and epoch 10/10, Training Loss: 2.990376988877374e-05, Validation Loss: 5.2131612225802985e-05\n",
            "for the 25 th iteration and epoch 1/10, Training Loss: 1.1859285823037184, Validation Loss: 1.556344899473802\n",
            "for the 25 th iteration and epoch 4/10, Training Loss: 1.0390715270637947, Validation Loss: 1.5824751596308926\n",
            "for the 25 th iteration and epoch 7/10, Training Loss: 1.0060851885699433, Validation Loss: 1.5069501627690578\n",
            "for the 25 th iteration and epoch 10/10, Training Loss: 1.0337309228615534, Validation Loss: 1.5507436252352647\n",
            "for the 25 th iteration and epoch 1/10, Training Loss: 2.5040911834735035e-05, Validation Loss: 1.0736809026725809e-05\n",
            "for the 25 th iteration and epoch 4/10, Training Loss: 1.5898923735947105e-05, Validation Loss: 1.0294760314044495e-05\n",
            "for the 25 th iteration and epoch 7/10, Training Loss: 1.0412028418000617e-05, Validation Loss: 6.855342856124853e-06\n",
            "for the 25 th iteration and epoch 10/10, Training Loss: 7.063324989902507e-06, Validation Loss: 1.2112051387328714e-05\n",
            "for the 24 th iteration and epoch 1/10, Training Loss: 1.1114065304957985, Validation Loss: 1.1722211737192216\n",
            "for the 24 th iteration and epoch 4/10, Training Loss: 1.1433353409421114, Validation Loss: 1.187916819215074\n",
            "for the 24 th iteration and epoch 7/10, Training Loss: 1.036788325066023, Validation Loss: 1.1821924013202612\n",
            "for the 24 th iteration and epoch 10/10, Training Loss: 1.0574141126207346, Validation Loss: 1.179130530901239\n",
            "for the 24 th iteration and epoch 1/10, Training Loss: 1.3084718568738026e-05, Validation Loss: 2.229102018783101e-05\n",
            "for the 24 th iteration and epoch 4/10, Training Loss: 1.1673365615754799e-05, Validation Loss: 5.672626018094386e-06\n",
            "for the 24 th iteration and epoch 7/10, Training Loss: 6.739559795257397e-06, Validation Loss: 5.305967483901799e-06\n",
            "for the 24 th iteration and epoch 10/10, Training Loss: 6.106744532469423e-06, Validation Loss: 7.3716625256687964e-06\n",
            "for the 23 th iteration and epoch 1/10, Training Loss: 1.012910995020654, Validation Loss: 1.7082530756829246\n",
            "for the 23 th iteration and epoch 4/10, Training Loss: 1.008956298468259, Validation Loss: 1.7635786959431023\n",
            "for the 23 th iteration and epoch 7/10, Training Loss: 1.023744992543981, Validation Loss: 1.6100764343137963\n",
            "for the 23 th iteration and epoch 10/10, Training Loss: 1.1857932358754897, Validation Loss: 1.884380001006173\n",
            "for the 23 th iteration and epoch 1/10, Training Loss: 2.352233534747405e-05, Validation Loss: 4.278402408888431e-05\n",
            "for the 23 th iteration and epoch 4/10, Training Loss: 7.17897532650151e-06, Validation Loss: 8.69267365489227e-06\n",
            "for the 23 th iteration and epoch 7/10, Training Loss: 6.545288049313418e-06, Validation Loss: 6.0275583390391374e-06\n",
            "for the 23 th iteration and epoch 10/10, Training Loss: 6.088106894936643e-06, Validation Loss: 2.467599961441312e-05\n",
            "for the 22 th iteration and epoch 1/10, Training Loss: 1.232853996361311, Validation Loss: 1.2123679020916978\n",
            "for the 22 th iteration and epoch 4/10, Training Loss: 1.2501673345867377, Validation Loss: 1.128737530236799\n",
            "for the 22 th iteration and epoch 7/10, Training Loss: 1.0440848181359794, Validation Loss: 1.103657247155867\n",
            "for the 22 th iteration and epoch 10/10, Training Loss: 1.0585778095456675, Validation Loss: 1.1283985979271531\n",
            "for the 22 th iteration and epoch 1/10, Training Loss: 4.191372197618808e-05, Validation Loss: 4.1331883588683063e-05\n",
            "for the 22 th iteration and epoch 4/10, Training Loss: 1.1835083643452544e-05, Validation Loss: 2.0943223732707924e-05\n",
            "for the 22 th iteration and epoch 7/10, Training Loss: 6.1555181229132236e-06, Validation Loss: 1.1925827630604354e-05\n",
            "for the 22 th iteration and epoch 10/10, Training Loss: 5.726947167480413e-06, Validation Loss: 1.2400094789918878e-05\n",
            "for the 21 th iteration and epoch 1/10, Training Loss: 1.207648971233286, Validation Loss: 1.028287345534661\n",
            "for the 21 th iteration and epoch 4/10, Training Loss: 1.2768751670600411, Validation Loss: 0.9365513200131963\n",
            "for the 21 th iteration and epoch 7/10, Training Loss: 1.115073189297824, Validation Loss: 0.97427746912416\n",
            "for the 21 th iteration and epoch 10/10, Training Loss: 1.1654408135624368, Validation Loss: 0.9370448612708178\n",
            "for the 21 th iteration and epoch 1/10, Training Loss: 1.1120483959019639e-05, Validation Loss: 8.505850385814904e-06\n",
            "for the 21 th iteration and epoch 4/10, Training Loss: 7.452236255089305e-06, Validation Loss: 6.19797049225612e-06\n",
            "for the 21 th iteration and epoch 7/10, Training Loss: 6.303980207090411e-06, Validation Loss: 6.5182445977265124e-06\n",
            "for the 21 th iteration and epoch 10/10, Training Loss: 3.915565581194878e-06, Validation Loss: 1.3114864757722236e-05\n",
            "for the 20 th iteration and epoch 1/10, Training Loss: 0.9978941900227374, Validation Loss: 1.059970120592567\n",
            "for the 20 th iteration and epoch 4/10, Training Loss: 1.1597304707104434, Validation Loss: 1.059911284648272\n",
            "for the 20 th iteration and epoch 7/10, Training Loss: 1.0313955821026801, Validation Loss: 1.0611567440421479\n",
            "for the 20 th iteration and epoch 10/10, Training Loss: 1.2704715193607306, Validation Loss: 1.0683624800900628\n",
            "for the 20 th iteration and epoch 1/10, Training Loss: 7.355979739262885e-06, Validation Loss: 6.164301994693489e-06\n",
            "for the 20 th iteration and epoch 4/10, Training Loss: 6.364802792905534e-06, Validation Loss: 7.850494569390808e-06\n",
            "for the 20 th iteration and epoch 7/10, Training Loss: 6.235754565144938e-06, Validation Loss: 3.7245126242850415e-06\n",
            "for the 20 th iteration and epoch 10/10, Training Loss: 5.202150164846244e-06, Validation Loss: 6.526738017253837e-06\n",
            "for the 19 th iteration and epoch 1/10, Training Loss: 1.124525811196579, Validation Loss: 0.9884512164178432\n",
            "for the 19 th iteration and epoch 4/10, Training Loss: 1.1457110919253446, Validation Loss: 0.8044588318517418\n",
            "for the 19 th iteration and epoch 7/10, Training Loss: 1.1316278610670074, Validation Loss: 0.8809748796293608\n",
            "for the 19 th iteration and epoch 10/10, Training Loss: 1.0736232673864943, Validation Loss: 0.9009047417509386\n",
            "for the 19 th iteration and epoch 1/10, Training Loss: 1.414334135807342e-05, Validation Loss: 1.817587419466814e-05\n",
            "for the 19 th iteration and epoch 4/10, Training Loss: 7.400653531075682e-06, Validation Loss: 1.5244095382651576e-05\n",
            "for the 19 th iteration and epoch 7/10, Training Loss: 8.172745459933278e-06, Validation Loss: 1.0965724766008662e-05\n",
            "for the 19 th iteration and epoch 10/10, Training Loss: 5.952750260877389e-06, Validation Loss: 9.96742936657734e-06\n",
            "for the 18 th iteration and epoch 1/10, Training Loss: 1.0615330053596757, Validation Loss: 1.3934661740855525\n",
            "for the 18 th iteration and epoch 4/10, Training Loss: 1.0673095633341618, Validation Loss: 1.412278575085731\n",
            "for the 18 th iteration and epoch 7/10, Training Loss: 1.065183641172624, Validation Loss: 1.4263737206643132\n",
            "for the 18 th iteration and epoch 10/10, Training Loss: 1.138719721329507, Validation Loss: 1.4057318682385307\n",
            "for the 18 th iteration and epoch 1/10, Training Loss: 5.834541751866234e-06, Validation Loss: 5.037237950077883e-06\n",
            "for the 18 th iteration and epoch 4/10, Training Loss: 5.270988842206282e-06, Validation Loss: 4.996978644051049e-06\n",
            "for the 18 th iteration and epoch 7/10, Training Loss: 4.654609309840228e-06, Validation Loss: 6.624931727122885e-06\n",
            "for the 18 th iteration and epoch 10/10, Training Loss: 1.0102197447422676e-05, Validation Loss: 1.964994905260492e-05\n",
            "for the 17 th iteration and epoch 1/10, Training Loss: 1.1852058988502523, Validation Loss: 0.8054301703422768\n",
            "for the 17 th iteration and epoch 4/10, Training Loss: 1.0911371436685338, Validation Loss: 0.7187234870593666\n",
            "for the 17 th iteration and epoch 7/10, Training Loss: 1.023062218859396, Validation Loss: 0.7342271661399149\n",
            "for the 17 th iteration and epoch 10/10, Training Loss: 1.0622009501181557, Validation Loss: 0.793415452014153\n",
            "for the 17 th iteration and epoch 1/10, Training Loss: 2.2485023985841893e-05, Validation Loss: 2.401312182582361e-06\n",
            "for the 17 th iteration and epoch 4/10, Training Loss: 7.111975680206204e-06, Validation Loss: 4.599807520582696e-06\n",
            "for the 17 th iteration and epoch 7/10, Training Loss: 5.598633469094605e-06, Validation Loss: 3.0075137075677436e-06\n",
            "for the 17 th iteration and epoch 10/10, Training Loss: 8.025337830297687e-06, Validation Loss: 4.191554835995322e-06\n",
            "for the 16 th iteration and epoch 1/10, Training Loss: 1.0525688425950581, Validation Loss: 1.0671838914697176\n",
            "for the 16 th iteration and epoch 4/10, Training Loss: 0.9495837841280871, Validation Loss: 1.0693747764402168\n",
            "for the 16 th iteration and epoch 7/10, Training Loss: 1.0391922005563743, Validation Loss: 1.1006958727621559\n",
            "for the 16 th iteration and epoch 10/10, Training Loss: 0.9222777295936107, Validation Loss: 1.1094855778233732\n",
            "for the 16 th iteration and epoch 1/10, Training Loss: 5.032321436633979e-06, Validation Loss: 3.5824671692522445e-06\n",
            "for the 16 th iteration and epoch 4/10, Training Loss: 3.396104643154849e-06, Validation Loss: 6.944445729025127e-06\n",
            "for the 16 th iteration and epoch 7/10, Training Loss: 4.049262386879795e-06, Validation Loss: 6.167494300371177e-06\n",
            "for the 16 th iteration and epoch 10/10, Training Loss: 4.825711372626782e-06, Validation Loss: 4.399477267903837e-06\n",
            "for the 15 th iteration and epoch 1/10, Training Loss: 0.9155784353924634, Validation Loss: 1.1144691135275195\n",
            "for the 15 th iteration and epoch 4/10, Training Loss: 0.9578896386189631, Validation Loss: 1.0835595011764978\n",
            "for the 15 th iteration and epoch 7/10, Training Loss: 0.8496251690667543, Validation Loss: 1.0949939698435511\n",
            "for the 15 th iteration and epoch 10/10, Training Loss: 0.919007580004814, Validation Loss: 1.0838172455094957\n",
            "for the 15 th iteration and epoch 1/10, Training Loss: 2.3725563769017285e-06, Validation Loss: 1.6080170396310209e-06\n",
            "for the 15 th iteration and epoch 4/10, Training Loss: 2.1254859366904827e-06, Validation Loss: 1.5216891111372733e-06\n",
            "for the 15 th iteration and epoch 7/10, Training Loss: 3.710812909587505e-06, Validation Loss: 3.419897252122604e-06\n",
            "for the 15 th iteration and epoch 10/10, Training Loss: 1.8654627420238858e-06, Validation Loss: 1.2381156583318263e-06\n",
            "for the 14 th iteration and epoch 1/10, Training Loss: 0.8725814812669798, Validation Loss: 1.2868237256747617\n",
            "for the 14 th iteration and epoch 4/10, Training Loss: 0.8547250373664552, Validation Loss: 1.3551682527828643\n",
            "for the 14 th iteration and epoch 7/10, Training Loss: 0.8445462164240191, Validation Loss: 1.3072080967919582\n",
            "for the 14 th iteration and epoch 10/10, Training Loss: 0.8487631756012606, Validation Loss: 1.2573592018306186\n",
            "for the 14 th iteration and epoch 1/10, Training Loss: 2.1138650544863137e-06, Validation Loss: 1.9117163943139526e-06\n",
            "for the 14 th iteration and epoch 4/10, Training Loss: 3.7901898373135284e-06, Validation Loss: 1.3293397856769561e-05\n",
            "for the 14 th iteration and epoch 7/10, Training Loss: 4.228749717633879e-06, Validation Loss: 2.152284378837999e-06\n",
            "for the 14 th iteration and epoch 10/10, Training Loss: 2.7226098280930843e-06, Validation Loss: 1.6020333809122242e-06\n",
            "for the 13 th iteration and epoch 1/10, Training Loss: 0.8344942989006228, Validation Loss: 1.1987590520495255\n",
            "for the 13 th iteration and epoch 4/10, Training Loss: 0.8930690548658307, Validation Loss: 1.1908243250932098\n",
            "for the 13 th iteration and epoch 7/10, Training Loss: 0.8568680528812056, Validation Loss: 1.2254505694790236\n",
            "for the 13 th iteration and epoch 10/10, Training Loss: 0.8249626579084307, Validation Loss: 1.243153284201675\n",
            "for the 13 th iteration and epoch 1/10, Training Loss: 9.957087544699723e-06, Validation Loss: 3.1574203128810464e-06\n",
            "for the 13 th iteration and epoch 4/10, Training Loss: 3.1908442141287584e-06, Validation Loss: 5.565870038544887e-06\n",
            "for the 13 th iteration and epoch 7/10, Training Loss: 2.405879028517326e-06, Validation Loss: 5.548942666185523e-06\n",
            "for the 13 th iteration and epoch 10/10, Training Loss: 1.5850655636635059e-06, Validation Loss: 3.101432303165872e-06\n",
            "for the 12 th iteration and epoch 1/10, Training Loss: 0.7996815732540719, Validation Loss: 0.9841616342665989\n",
            "for the 12 th iteration and epoch 4/10, Training Loss: 0.8999265271798403, Validation Loss: 0.9260898932789953\n",
            "for the 12 th iteration and epoch 7/10, Training Loss: 0.8473801084210942, Validation Loss: 0.9756117715164392\n",
            "for the 12 th iteration and epoch 10/10, Training Loss: 0.8167814856387501, Validation Loss: 0.9638523660195855\n",
            "for the 12 th iteration and epoch 1/10, Training Loss: 1.207675476325917e-05, Validation Loss: 1.0064940775687208e-05\n",
            "for the 12 th iteration and epoch 4/10, Training Loss: 4.140754929178509e-06, Validation Loss: 8.308988460834065e-06\n",
            "for the 12 th iteration and epoch 7/10, Training Loss: 1.8879192671690386e-06, Validation Loss: 3.0179408578525973e-06\n",
            "for the 12 th iteration and epoch 10/10, Training Loss: 1.3369282859574455e-06, Validation Loss: 6.766218410953697e-07\n",
            "for the 11 th iteration and epoch 1/10, Training Loss: 0.7148164726765069, Validation Loss: 1.6875717441455358\n",
            "for the 11 th iteration and epoch 4/10, Training Loss: 0.7060969444485762, Validation Loss: 1.6961262820699687\n",
            "for the 11 th iteration and epoch 7/10, Training Loss: 0.7013913101413091, Validation Loss: 1.6982905573512184\n",
            "for the 11 th iteration and epoch 10/10, Training Loss: 0.6648551336501158, Validation Loss: 1.688484821530971\n",
            "for the 11 th iteration and epoch 1/10, Training Loss: 3.8697605407153416e-06, Validation Loss: 2.473833860050704e-06\n",
            "for the 11 th iteration and epoch 4/10, Training Loss: 1.3865237891200622e-06, Validation Loss: 4.491501565335824e-06\n",
            "for the 11 th iteration and epoch 7/10, Training Loss: 6.345384124209684e-07, Validation Loss: 5.261477733950452e-07\n",
            "for the 11 th iteration and epoch 10/10, Training Loss: 5.132087538260759e-07, Validation Loss: 6.098749586928472e-07\n",
            "for the 10 th iteration and epoch 1/10, Training Loss: 0.6833176069736486, Validation Loss: 1.2463938853414307\n",
            "for the 10 th iteration and epoch 4/10, Training Loss: 0.6881828357805054, Validation Loss: 1.3349160843970975\n",
            "for the 10 th iteration and epoch 7/10, Training Loss: 0.7305912878048885, Validation Loss: 1.2255491151546178\n",
            "for the 10 th iteration and epoch 10/10, Training Loss: 0.6609528147489319, Validation Loss: 1.3192426130028607\n",
            "for the 10 th iteration and epoch 1/10, Training Loss: 4.242238365364712e-05, Validation Loss: 3.2408047612200646e-05\n",
            "for the 10 th iteration and epoch 4/10, Training Loss: 6.8933446996638665e-06, Validation Loss: 4.302074619675867e-06\n",
            "for the 10 th iteration and epoch 7/10, Training Loss: 6.696514325823033e-07, Validation Loss: 1.0817782768206836e-06\n",
            "for the 10 th iteration and epoch 10/10, Training Loss: 1.695829694468362e-07, Validation Loss: 2.3367545700890593e-08\n",
            "for the 9 th iteration and epoch 1/10, Training Loss: 0.7061395946475362, Validation Loss: 0.8527489556688765\n",
            "for the 9 th iteration and epoch 4/10, Training Loss: 0.6940741053743625, Validation Loss: 0.8425038102110023\n",
            "for the 9 th iteration and epoch 7/10, Training Loss: 0.8737175574959197, Validation Loss: 0.8774643319048625\n",
            "for the 9 th iteration and epoch 10/10, Training Loss: 0.7331409639889342, Validation Loss: 0.8514366829175408\n",
            "for the 9 th iteration and epoch 1/10, Training Loss: 2.9232691205405738e-05, Validation Loss: 1.786836797248115e-05\n",
            "for the 9 th iteration and epoch 4/10, Training Loss: 2.7801218497177328e-06, Validation Loss: 1.8208629381593767e-06\n",
            "for the 9 th iteration and epoch 7/10, Training Loss: 8.552552073718209e-07, Validation Loss: 2.8840287244054114e-06\n",
            "for the 9 th iteration and epoch 10/10, Training Loss: 1.0525505672025673e-06, Validation Loss: 4.093405997330757e-07\n",
            "for the 8 th iteration and epoch 1/10, Training Loss: 0.5618158135892781, Validation Loss: 1.3886618152324133\n",
            "for the 8 th iteration and epoch 4/10, Training Loss: 0.5491239517362214, Validation Loss: 1.3577061518272813\n",
            "for the 8 th iteration and epoch 7/10, Training Loss: 0.53468458549037, Validation Loss: 1.3854166832568242\n",
            "for the 8 th iteration and epoch 10/10, Training Loss: 0.5889636439396883, Validation Loss: 1.3787185635642099\n",
            "for the 8 th iteration and epoch 1/10, Training Loss: 1.657027092673419e-05, Validation Loss: 1.5617123861663635e-05\n",
            "for the 8 th iteration and epoch 4/10, Training Loss: 3.952766516954984e-06, Validation Loss: 4.698746510498752e-06\n",
            "for the 8 th iteration and epoch 7/10, Training Loss: 1.1154530084876533e-06, Validation Loss: 2.8934205972569654e-06\n",
            "for the 8 th iteration and epoch 10/10, Training Loss: 6.680369006881162e-07, Validation Loss: 1.155904933555351e-06\n",
            "for the 7 th iteration and epoch 1/10, Training Loss: 0.5139425393130228, Validation Loss: 0.6632842903218985\n",
            "for the 7 th iteration and epoch 4/10, Training Loss: 0.5305674562687064, Validation Loss: 0.6595876564088813\n",
            "for the 7 th iteration and epoch 7/10, Training Loss: 0.5436287769419914, Validation Loss: 0.6620267689200836\n",
            "for the 7 th iteration and epoch 10/10, Training Loss: 0.5308154605416509, Validation Loss: 0.6601355673741462\n",
            "for the 7 th iteration and epoch 1/10, Training Loss: 2.899308820127871e-06, Validation Loss: 1.0985771763964199e-05\n",
            "for the 7 th iteration and epoch 4/10, Training Loss: 8.405228802376132e-07, Validation Loss: 8.492373225094747e-07\n",
            "for the 7 th iteration and epoch 7/10, Training Loss: 1.1527166875279544e-06, Validation Loss: 1.1922765236564188e-06\n",
            "for the 7 th iteration and epoch 10/10, Training Loss: 1.191460391950293e-06, Validation Loss: 1.7311207133630107e-06\n",
            "for the 6 th iteration and epoch 1/10, Training Loss: 0.49984895398865736, Validation Loss: 0.576597128845052\n",
            "for the 6 th iteration and epoch 4/10, Training Loss: 0.43186382827555825, Validation Loss: 0.6317043383148987\n",
            "for the 6 th iteration and epoch 7/10, Training Loss: 0.4492598011902468, Validation Loss: 0.6239641636431867\n",
            "for the 6 th iteration and epoch 10/10, Training Loss: 0.43001659159797256, Validation Loss: 0.646321998117094\n",
            "for the 6 th iteration and epoch 1/10, Training Loss: 1.2734441712256487e-06, Validation Loss: 5.833486161292274e-07\n",
            "for the 6 th iteration and epoch 4/10, Training Loss: 1.54998807395278e-06, Validation Loss: 9.755901377282712e-07\n",
            "for the 6 th iteration and epoch 7/10, Training Loss: 1.938512904436345e-06, Validation Loss: 3.333471126016991e-06\n",
            "for the 6 th iteration and epoch 10/10, Training Loss: 1.0029581447729283e-06, Validation Loss: 1.227600330290312e-06\n",
            "for the 5 th iteration and epoch 1/10, Training Loss: 0.4309689335504275, Validation Loss: 0.43808069123470383\n",
            "for the 5 th iteration and epoch 4/10, Training Loss: 0.4572192565025971, Validation Loss: 0.43877357459607713\n",
            "for the 5 th iteration and epoch 7/10, Training Loss: 0.4225774956050642, Validation Loss: 0.5393210478288164\n",
            "for the 5 th iteration and epoch 10/10, Training Loss: 0.46126082801506874, Validation Loss: 0.4541956304014677\n",
            "for the 5 th iteration and epoch 1/10, Training Loss: 2.689930943839191e-05, Validation Loss: 5.85537309591116e-05\n",
            "for the 5 th iteration and epoch 4/10, Training Loss: 4.554230280273239e-06, Validation Loss: 7.348840328434465e-06\n",
            "for the 5 th iteration and epoch 7/10, Training Loss: 5.686725048650169e-07, Validation Loss: 1.5882425712988032e-06\n",
            "for the 5 th iteration and epoch 10/10, Training Loss: 1.4819726995987564e-07, Validation Loss: 2.674520706399176e-07\n",
            "for the 4 th iteration and epoch 1/10, Training Loss: 0.4465502772266513, Validation Loss: 0.19139715794468093\n",
            "for the 4 th iteration and epoch 4/10, Training Loss: 0.4338898352156825, Validation Loss: 0.1764893966803459\n",
            "for the 4 th iteration and epoch 7/10, Training Loss: 0.4685083163369256, Validation Loss: 0.28718137452737774\n",
            "for the 4 th iteration and epoch 10/10, Training Loss: 0.4669119717683282, Validation Loss: 0.2180339348880155\n",
            "for the 4 th iteration and epoch 1/10, Training Loss: 7.135973042047013e-05, Validation Loss: 0.00010049697926525859\n",
            "for the 4 th iteration and epoch 4/10, Training Loss: 1.0860352555062295e-05, Validation Loss: 1.4240136371717249e-05\n",
            "for the 4 th iteration and epoch 7/10, Training Loss: 4.324638477775207e-06, Validation Loss: 3.408260215502018e-06\n",
            "for the 4 th iteration and epoch 10/10, Training Loss: 2.594600900082347e-06, Validation Loss: 6.9795450934313835e-06\n",
            "for the 3 th iteration and epoch 1/10, Training Loss: 0.2995488581537037, Validation Loss: 0.32353540204065545\n",
            "for the 3 th iteration and epoch 4/10, Training Loss: 0.2791977315748172, Validation Loss: 0.3085506399592317\n",
            "for the 3 th iteration and epoch 7/10, Training Loss: 0.2890205211918673, Validation Loss: 0.3298011641397198\n",
            "for the 3 th iteration and epoch 10/10, Training Loss: 0.31322299161254175, Validation Loss: 0.3298596402651472\n",
            "for the 3 th iteration and epoch 1/10, Training Loss: 2.819471590408264e-06, Validation Loss: 7.074711059698626e-06\n",
            "for the 3 th iteration and epoch 4/10, Training Loss: 1.2139558995536114e-06, Validation Loss: 5.4214047864527456e-06\n",
            "for the 3 th iteration and epoch 7/10, Training Loss: 1.5720079599526757e-06, Validation Loss: 3.2402659900241402e-06\n",
            "for the 3 th iteration and epoch 10/10, Training Loss: 8.144455901337731e-07, Validation Loss: 1.5318072663512192e-06\n",
            "for the 2 th iteration and epoch 1/10, Training Loss: 0.2799585967352426, Validation Loss: 0.2628468901560387\n",
            "for the 2 th iteration and epoch 4/10, Training Loss: 0.2733568013745155, Validation Loss: 0.27400940876092594\n",
            "for the 2 th iteration and epoch 7/10, Training Loss: 0.25003300539996276, Validation Loss: 0.2712066634373349\n",
            "for the 2 th iteration and epoch 10/10, Training Loss: 0.2875143675031072, Validation Loss: 0.26173621728286645\n",
            "for the 2 th iteration and epoch 1/10, Training Loss: 6.006498977559483e-07, Validation Loss: 4.1668192627087764e-07\n",
            "for the 2 th iteration and epoch 4/10, Training Loss: 9.6684622435482e-07, Validation Loss: 4.204243617561041e-07\n",
            "for the 2 th iteration and epoch 7/10, Training Loss: 1.1935744285289065e-06, Validation Loss: 3.593765657403233e-07\n",
            "for the 2 th iteration and epoch 10/10, Training Loss: 2.150466566285209e-06, Validation Loss: 3.0181634828660358e-06\n",
            "for the 1 th iteration and epoch 1/10, Training Loss: 0.2098692752569256, Validation Loss: 0.5344917070329422\n",
            "for the 1 th iteration and epoch 4/10, Training Loss: 0.2125625621746947, Validation Loss: 0.5482993787940434\n",
            "for the 1 th iteration and epoch 7/10, Training Loss: 0.2146237861852025, Validation Loss: 0.5469026101208673\n",
            "for the 1 th iteration and epoch 10/10, Training Loss: 0.1925769059646436, Validation Loss: 0.5491241258042058\n",
            "for the 1 th iteration and epoch 1/10, Training Loss: 6.005857217352384e-06, Validation Loss: 1.2656128035613767e-05\n",
            "for the 1 th iteration and epoch 4/10, Training Loss: 9.746184451037711e-07, Validation Loss: 2.038369881693356e-06\n",
            "for the 1 th iteration and epoch 7/10, Training Loss: 4.135348236075602e-07, Validation Loss: 3.1898306606894247e-07\n",
            "for the 1 th iteration and epoch 10/10, Training Loss: 5.29792594177837e-07, Validation Loss: 4.358848939273161e-07\n",
            "for the 0 th iteration and epoch 1/10, Training Loss: 0.21532301153251635, Validation Loss: 0.1687702566257109\n",
            "for the 0 th iteration and epoch 4/10, Training Loss: 0.21435949233861679, Validation Loss: 0.16988335899522225\n",
            "for the 0 th iteration and epoch 7/10, Training Loss: 0.21837219565015906, Validation Loss: 0.16798579178412076\n",
            "for the 0 th iteration and epoch 10/10, Training Loss: 0.22117496595708475, Validation Loss: 0.1719062541005271\n",
            "for the 0 th iteration and epoch 1/10, Training Loss: 4.298248169559254e-06, Validation Loss: 7.794253980592158e-06\n",
            "for the 0 th iteration and epoch 4/10, Training Loss: 6.34270711438049e-07, Validation Loss: 2.2715803292915204e-07\n",
            "for the 0 th iteration and epoch 7/10, Training Loss: 1.1484083478082572e-07, Validation Loss: 1.0831684308057969e-07\n",
            "for the 0 th iteration and epoch 10/10, Training Loss: 1.5256583696619803e-08, Validation Loss: 1.2146735442850108e-08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "eNe1fLG9_xnB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "980c50d3-0996-419e-8a3b-3cdda425a180"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7.248307879543311\n",
            "3.990814446003303\n"
          ]
        }
      ],
      "source": [
        "#Monté Carlo\n",
        "u0=torch.mean(X[:,0]).cpu().numpy()\n",
        "l0=torch.mean(Y[:,0]).cpu().numpy()\n",
        "print(u0)\n",
        "print(l0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "dlx9jG8GSdJ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3d2797a-7898-4d8e-c822-6e0289e3049d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.2574934335400076\n",
            "0.01249999999999929\n",
            "3.2449934335400084\n"
          ]
        }
      ],
      "source": [
        "print(np.abs(u0-l0))\n",
        "print(4.4887-4.4762)\n",
        "print(np.abs(np.abs(u0-l0)-(4.4887-4.4762)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}