{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgNsRaRyoeBD",
        "outputId": "c23917e2-c11d-4ac8-e3c7-9de459834589"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "\n",
        "USE_GPU = True\n",
        "\n",
        "dtype = torch.float64 # we will be using float throughout this tutorial\n",
        "device = torch.device('cuda') if (USE_GPU and torch.cuda.is_available()) else torch.device('cpu')\n",
        "print('using device:', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQ_yOEOQ_bsD"
      },
      "source": [
        "## American Option Pricing with Multiple Neural Networks (method 1) article [1]\n",
        "Here I'll try a simple implementation of the method I of the first article :\n",
        "\n",
        "Here we have constant interest rate so the discount factor is $\\exp(-rT)$, and the stock dynamics are modelled with Geometric Brownian Motion (GBM).\n",
        "\n",
        "$\\large dS_t = rS_tdt+\\sigma S_tdW_t$\n",
        "\n",
        "Let's simulate this GBM process by simulating variables of the natural logarithm process of the stock price $x_t = ln(S_t)$, which is normally distributed. For the dynamics of the natural logarithm process of stock prices under GBM model you need to use Ito's calculus.\n",
        "$\\large dx_t = \\nu dt+\\sigma dW_t ,  \\nu = r - \\frac{1}{2} \\sigma ^ 2$\n",
        "\n",
        "We can then discretize the Stochastic Differential Equation (SDE) by changing the infinitesimals $dx, dt, dz$ into small steps $\\Delta x, \\Delta t, \\Delta W$.\n",
        "\n",
        "$\\large \\Delta x = \\nu  \\Delta t+\\sigma \\Delta W$\n",
        "\n",
        "This is the exact solution to the SDE and involves no approximation.\n",
        "\n",
        "$\\large x_{t+\\Delta t} = x_{t} + \\nu (\\Delta t)+\\sigma (W_{t+\\Delta t}- W_t)$\n",
        "\n",
        "In terms of the stock price S, we have:\n",
        "\n",
        "$\\large S_{t+\\Delta t} = S_{t} \\exp( \\nu \\Delta t + \\sigma (W_{t+\\Delta t}- W_t) )$\n",
        "\n",
        "Where $(W_{t+\\Delta t}- W_t) \\sim N(0,\\Delta t) \\sim \\sqrt{\\Delta t} N(0,1) \\sim \\sqrt{\\Delta t} \\epsilon_i$\n",
        "\n",
        "\n",
        "\\\\\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-aSQ4Kp_fgD"
      },
      "outputs": [],
      "source": [
        "def pay_off(S,r,K,dt):\n",
        "  return torch.maximum(K- S,torch.zeros_like(S))\n",
        "\n",
        "def simulate_paths(M, T, n, r, vol, S, K, dt):\n",
        "    nudt = (r - 0.5 * vol**2) * dt\n",
        "    lnS = np.log(S)\n",
        "\n",
        "    # Méthode de Monte Carlo\n",
        "    Z = np.random.normal(size=(M,n))\n",
        "    dW=np.sqrt(dt) * Z  #it's the simulation of the dWt_i we'll need in each iteration\n",
        "    delta_lnSt = nudt + vol*dW\n",
        "\n",
        "    LnS_s = np.zeros([M, n + 1])\n",
        "\n",
        "    # Set the initial values\n",
        "    LnS_s[:, 0] = lnS\n",
        "    # Compute cumulative sums using cumsum\n",
        "    LnS_s[:, 1:] = np.cumsum(delta_lnSt, axis=1)\n",
        "    LnS_s[:,1:] +=lnS\n",
        "\n",
        "\n",
        "    S = np.exp(LnS_s)\n",
        "    S_tensor = torch.tensor(S, device = device ,dtype=dtype)\n",
        "    dW_tensor = torch.tensor(dW, device = device ,dtype=dtype)\n",
        "    return S_tensor, dW_tensor\n",
        "\n",
        "#Parametres\n",
        "\n",
        "T = 1\n",
        "n=50\n",
        "dt = T/n  #les t_i seront donc les i*dt.\n",
        "\n",
        "\n",
        "S = 36          # Prix de l'action\n",
        "K = 40           # Prix d'exercice\n",
        "vol = 0.2       # Volatilité (%)\n",
        "r = 0.06            # Taux sans risque (%)\n",
        "M = 100000 # Nombre de simulations\n",
        "\n",
        "\n",
        "S,dW=simulate_paths(M, T, n, r, vol, S, K, dt)\n",
        "X=torch.zeros([M,n+1], device = device ,dtype=dtype)\n",
        "Y=torch.zeros_like(X, device = device ,dtype=dtype)\n",
        "\n",
        "\n",
        "X[ :, n]=pay_off(S[:,n],r,K,dt)\n",
        "Y[ :, n]=X[ :, n]\n",
        "\n",
        "beta_dt=math.exp(-r*dt)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbAi3cLQjcBN"
      },
      "source": [
        "# The algorithm :\n",
        "\n",
        "***Algorithm 1 :*** American Option Pricing with Multiple Neural Networks\n",
        "\n",
        "**Result :** Functions $\\Phi_{t_i}, \\Psi_{t_i}$ for $i \\in \\{0,1, \\ldots, n-1\\}$\n",
        "\n",
        "Simulate $N$ stock paths\n",
        "\n",
        "Initialize $Y_{t_n}=X_{t_n}=f\\left(S_{t_n}\\right)$\n",
        "\n",
        "for $i=n-1: 1$ do \\\\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; Regress $\\beta_{\\Delta t}^{-1} Y_{t_{i+1}}$ on $S_{t_i}:$ \\\\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\min_{\\Phi_{t_i}, \\Psi_{t_i}}\\left(\\beta_{\\Delta t}^{-1} Y_{t_{i+1}} - \\Phi_{t_i}\\left(S_{t_i}\\right) - \\Psi_{t_i}\\left(S_{t_i}\\right) \\Delta W_{t_i}\\right)^2$ \\\\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; $Y_{t_i} = \\beta_{\\Delta t}^{-1} Y_{t_{i+1}} - \\Psi_{t_i}\\left(S_{t_i}\\right) \\Delta W_{t_i}$ \\\\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; $X_{t_i} = \\beta_{\\Delta t}^{-1} X_{t_{i+1}} - \\Psi_{t_i}\\left(S_{t_i}\\right) \\Delta W_{t_i}$ \\\\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; if $f\\left(S_{t_i}\\right) > \\Phi_{t_i}\\left(S_{t_i}\\right)$ then \\\\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $Y_{t_i} = f\\left(S_{t_i}\\right)$ \\\\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; end \\\\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; if $f\\left(S_{t_i}\\right) > X_{t_i}$ then \\\\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $X_{t_i} = f\\left(S_{t_i}\\right)$ \\\\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; end \\\\\n",
        "end\n",
        "\n",
        "Regress $\\beta_{\\Delta t}^{-1} Y_{t_1}$ on $S_{t_0}:$ \\\\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; $\\min \\left(\\beta_{\\Delta t}^{-1} Y_{t_1} - \\Phi_{t_0}\\left(S_{t_0}\\right) - \\Psi_{t_0}\\left(S_{t_0}\\right) \\Delta W_{t_0}\\right)^2$ \\\\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; $Y_{t_0} = \\beta_{\\Delta t}^{-1} Y_{t_1} - \\Psi_{t_0}\\left(S_{t_0}\\right) \\Delta W_{t_0}$ \\\\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; $X_{t_0} = \\beta_{\\Delta t}^{-1} X_{t_1} - \\Psi_{t_0}\\left(S_{t_0}\\right) \\Delta W_{t_0}$\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "class SpecificModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SpecificModel, self).__init__()\n",
        "        self.branch1 = nn.Sequential(\n",
        "            nn.Linear(1, 50),\n",
        "            # nn.BatchNorm1d(50),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(50, 50),\n",
        "            # nn.BatchNorm1d(50),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(50, 1)\n",
        "        )\n",
        "        self.branch2 = nn.Sequential(\n",
        "            nn.Linear(1, 50),\n",
        "            # nn.BatchNorm1d(50),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(50, 50),\n",
        "            # nn.BatchNorm1d(30),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(50, 50),\n",
        "            # nn.BatchNorm1d(30),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(50, 1)\n",
        "        )\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=0.01)\n",
        "        # self.optimizer = optim.SGD(self.parameters(), lr=0.01), momentum=0.9)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_branch1 = self.branch1(x)\n",
        "        y_branch2 = self.branch2(x)\n",
        "        concatenated_output = torch.cat((x_branch1, y_branch2), dim=1)\n",
        "        return concatenated_output\n",
        "\n",
        "    def custom_loss(self, Y_true, dW_true, y_pred):\n",
        "        return torch.mean(torch.square(Y_true - (y_pred[:, 0] + dW_true * y_pred[:, 1])))\n",
        "\n",
        "    def train_model(self, S_tensor, Y_tensor, dW_tensor, epochs=5, batch_size=32, validation_split=0.2):\n",
        "        torch.cuda.empty_cache()\n",
        "        combined_labels = torch.cat((Y_tensor.unsqueeze(1), dW_tensor.unsqueeze(1)), dim=1)\n",
        "        dataset = TensorDataset(S_tensor, combined_labels)\n",
        "        train_size = int((1 - validation_split) * len(dataset))\n",
        "        val_size = len(dataset) - train_size\n",
        "        train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self.train()\n",
        "            total_loss = 0\n",
        "            for S_batch, labels_batch in train_dataloader:\n",
        "                self.optimizer.zero_grad()\n",
        "                outputs = self(S_batch)\n",
        "                loss = self.custom_loss(labels_batch[:, 0], labels_batch[:, 1], outputs)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "\n",
        "\n",
        "            val_loss = 0\n",
        "\n",
        "            if epoch % 3 == 0:\n",
        "                self.eval()\n",
        "                with torch.no_grad():\n",
        "                    for S_batch, labels_batch in val_dataloader:\n",
        "                        outputs = self(S_batch)\n",
        "                        val_loss += self.custom_loss(labels_batch[:, 0], labels_batch[:, 1], outputs).item()\n",
        "\n",
        "                print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {total_loss/len(train_dataset)}, Validation Loss: {val_loss/val_size}\")\n",
        "\n",
        "\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "              outputs = self(S_tensor)\n",
        "        print(\"Training complete .\")\n",
        "\n",
        "        return outputs\n",
        "\n",
        "model = SpecificModel().to(device = device ,dtype=dtype)\n"
      ],
      "metadata": {
        "id": "O_Rr5mc237PA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cH8r8hu8xPh"
      },
      "outputs": [],
      "source": [
        "# The main loop :\n",
        "\n",
        "for i in range(n-1,0,-1):\n",
        "  #determination of phi and psi in one step\n",
        "  # Convert data to PyTorch tensors\n",
        "  # inputs = torch.tensor(S[:,i].reshape(-1,1),device = device ,dtype=dtype)\n",
        "  # scaler = StandardScaler()\n",
        "  # normalized_S = scaler.fit_transform(S[:,i].reshape(-1,1))\n",
        "  print( \"In the \",i ,\" th step :\")\n",
        "  outputs =  model.train_model(S[:,i].reshape(-1,1), beta_dt*Y[:,i+1].reshape(-1,1), dW[:,i].reshape(-1,1))\n",
        "\n",
        "  psi_S=outputs[:,1]\n",
        "  phi_S=outputs[:,0]\n",
        "\n",
        "  X[ :, i]=beta_dt*X[:,i+1]-psi_S *dW[:,i] #to change it into (M,)\n",
        "  Y[ :, i]=beta_dt*Y[:,i+1]-psi_S *dW[:,i]\n",
        "\n",
        "  Z=pay_off(S[:,i],r,K,dt)\n",
        "\n",
        "  Y[ :, i] = torch.where(Z> phi_S, Z, Y[ :, i])\n",
        "  X[ :, i] = torch.where(Z> X[:,i], Z, X[ :, i])\n",
        "\n",
        "\n",
        "#Pour i =0 :\n",
        "#determination of phi and psi in two steps\n",
        "i=0\n",
        "outputs = model.train_model(S[:,i].reshape(-1,1), beta_dt*Y[:,i+1].reshape(-1,1), dW[:,i].reshape(-1,1))\n",
        "\n",
        "psi_S=outputs[:,1]\n",
        "phi_S=outputs[:,0]\n",
        "\n",
        "X[ :, 0]=beta_dt*X[:,1]-psi_S *dW[:,0]\n",
        "Y[ :, 0]=beta_dt*Y[:,1]-psi_S *dW[:,0]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNe1fLG9_xnB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2bcf86e-fc31-4d75-95e6-9dc19a174eaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.859674071781852\n",
            "4.704045255605416\n"
          ]
        }
      ],
      "source": [
        "#Monté Carlo\n",
        "u0=torch.mean(X[:,0]).cpu().numpy()\n",
        "l0=torch.mean(Y[:,0]).cpu().numpy()\n",
        "print(u0)\n",
        "print(l0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpqgMaS4kHqL"
      },
      "source": [
        "With the same parameters and network structure  [(20,20),(20,20,20)] of page 17 in  article [1] , the results are :\n",
        "\n",
        "The upper bound we got is 4.565703035603622.\n",
        "\n",
        "The lower bound we got 4.437699059733613 .\n",
        "\n",
        "Article [1] showed that , in this case, the lower bound exhibits fluctuations with a mean of 4.4762, while the upper bound shows fluctuations with a mean of\n",
        "4.4887.\n",
        "\n",
        "\n",
        "We needed 2 hours to get this result!!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlx9jG8GSdJ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f4a980e-b03f-4e38-e9f4-ccf2adca7ae9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.155628816176436\n",
            "0.01249999999999929\n",
            "2.1431288161764366\n"
          ]
        }
      ],
      "source": [
        "print(np.abs(u0-l0))\n",
        "print(4.4887-4.4762)\n",
        "print(np.abs(np.abs(u0-l0)-(4.4887-4.4762)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KevBGiEVn0Xw"
      },
      "source": [
        "The difference (0.128 ) is what we wanted (0.0125 ) multiplied by 10, that's why we have to improve our code to get better results and in less time !!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}